{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36b7c047d9210b1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Chapter 3 Algorithm Analysis\n",
    "### 3.1 Experimental Studies\n",
    "We can use the elapsed time( by recording the time just before the algorithm and the time just after the algorithm and compute their differences) to demonstrate the time usage of a algorithm, for example:\n",
    "```python\n",
    "from time import time\n",
    "def time_measure(algorithm):\n",
    "    start_time = time() # record the starting time\n",
    "    algorithm()\n",
    "    return time() - start_time # compute and return the elapsed time.\n",
    "```\n",
    "#### Challenges of Experimental Analysis:\n",
    "1. It's hard to compare two different algorithm unless they are performed in the same software and hardware environment.\n",
    "2. Experiments can be done only on a limited set of test input; hence, they leave out the running time of some inputs which might be important.\n",
    "3. An algorithm must be fully implemented in order to execute it to study its running time experimentally.\n",
    "\n",
    "#### 3.1.1 Moving beyond Experimental Analysis\n",
    "Our goal is to develop an approach to analyzing the efficiency of algorithms that:\n",
    "1. Allows us to evaluate the relative efficiency of any two algorithms in a way that is dependent of the hardware and software environment.\n",
    "2. Is performed by studying a high-level description of the algorithm without need for implementation.\n",
    "3. Takes into account all possible inputs.\n",
    "\n",
    "Counting Primitive Operation\n",
    "To analyze the running time of an algorithm without performing experiments, we perform an analysis of directly on a high-level description of the algorithm (either in the form of the actual code fragment, or language-independent pseudo-code.) There are a set of primitive operations such as the following:\n",
    "\n",
    "- Assigning an identifier to an object\n",
    "- Determining the object associated with an identifier\n",
    "- Performing an arithmetic operation (for example, adding two numbers)\n",
    "- Comparing two numbers\n",
    "- Accessing a single element of a Python list by index\n",
    "- Calling a function (excluding operations executed within the function)\n",
    "- Returning from a function. \n",
    "\n",
    "These primitive operations correspond to a low-level instruction with an execute time that is constant. This might be the type of basic operation that is executed by hardware, even though many of our primitive operations may be translated to a small number of instructions, we can still consider that they can be counted by the amount of primitive operations are executed and use these number *t* as a measure of the running time of the algorithm.\n",
    "This *t* is more like a proportional number of the running time of that algorithm.\n",
    "\n",
    "#### Measuring Operations as a Function of input size\n",
    "For capturing the order of growth of an algorithm's running time. We will use f(n) to describe the number of primitive operation, and n represent the input size. \n",
    "\n",
    "#### Focusing on the Worst-Case input\n",
    "An algorithm may run faster on some inputs than it does on others. \n",
    "It's difficult to determine an average overall level of running time because that means we have to figure out the probability distribution on the set of inputs. Therefore, worst-case analysis is much easier than average-case analysis.\n",
    "Designing an algorithm to perform well in the worst cases leads to stronger algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da46332d4a8cf80",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 The seven function used in this book\n",
    "![Time complexity among different function](timecomplexity.jpg) \\\n",
    "\n",
    "#### 1. Constant Function *(f(n) = c)* :\n",
    "c refer to a constant value like 2 or 2^10^\n",
    "the formula might use a form *f(n) = cg(n) while g(n) is the most fundamental constant function g(n)=1* \n",
    "\n",
    "#### 2. Logarithm Function *($f(n) = log_b{n}$)*  \n",
    "$x = log_bn$ only if $b^x = n$\\\n",
    "the most common base is Logarithm Function is 2 because all integers are stored in computer as binary, and because a common operation in many algorithms is repeatedly divided an input in half. \n",
    "\n",
    "**Proposition3.1(Logarithm Rules)**: Given real numbers a > 0, b > 1, c > 0 and d > 1, we have:\\\n",
    " (* In this example, the usual convertion that the base is 2 if it is omitted)\n",
    " \n",
    "1. $log_b{(ac)} = log_b{a} + log_b{c}$ (e.g.: $log(2n) = log2 + logn = 1 + logn$) \n",
    "2. $logb(a/c) = log_b{a} - log_b{c}$ (e.g.: $log(n/2) = logn - log2 = logn-1$)\n",
    "3. $log_b{a^c} = clog_b{a}$ (e.g.: $logn^3 = 3logn$,  $log2^n = nlog2 = n · 1 = n$)\n",
    "4. **$log_b{a} = log_d{a}/log_d{b}$ (e.g.: $log~4~n = (logn)/log4 = (logn)/2$) *(This rule gives us a way to compute the base-two logarithn on a calculator that has a base-10 logarithm buttom.)*** \n",
    "5. $b^{log_d{a}} = a{log_d{b}}$ (e.g.: $2^{logn} = n^{log2} = n^1 = n$)\n",
    "\n",
    "#### 3. The Linear Function *($f(n) = n$, the best running time we can hope to achieve)*\n",
    "Another simple yet important function, the linear function *f*  assigns the given input value n itself.\\\n",
    "This function arises in algorithm analysis any time we have to do a single basic operation for each of n element. \\\n",
    "e.g. compare a number to each element of a sequence; \n",
    "\n",
    "#### 4. The N-log-N Function *($f(n) = nlogn$)* \n",
    "e.g. the fastest possible algorithms for sorting n arbitrary values require time proportional to nlogn.\n",
    "\n",
    "#### 5. The Quadratic Function *($f(n) = n^2$)*\n",
    "This function mainly is happened because there is nested loop while inner loop need a linear n times of operations and outer loop need another linear n times of operations as well, that is n * n operations. \\\n",
    "Another case of nested loop also can be described by Quadratic Function, where the first loop use one operation, the second loop use two operations, and so on （aka, Arithmetic Sequence). Cause $1 + 2 + 3 + ... + (n-1) + n = (n * (n-1)) /2 = (n^2 + n) /2$ (In this case, although the number of operations of inner loop just over half n, the order of growth is still quadratic n.\n",
    "\n",
    "#### 6. The Cubic Function and other Polynomials *($f(n) = n^3$, $f(n) = a_0+a_1n^1+a_2n^2+a_3n^3+\\dots+a_{d-1}n^{d-1}+a_dn^d$, )*\n",
    "***Polynomials***\n",
    "When $a_0,a_1,\\dots,a_{d-1},a_d$ are constants, we usually call them as coefficients and $a_d ≠ 0$. Integer d, which is the highest power in the polynomial, is called the degree of polynomial.\n",
    "And the running times that are polynomials with lower degree are generally better than polynomials with higher degree. \\\n",
    "***Summations***\n",
    "$\\sum_{i=a}^b f(i) = f(a+1) + f(a+2) + f(a+3) + ... + f(b)$ \\\n",
    "Summation notations give us a shorthand way to express the sums of increasing terms that have a regular structure:\\\n",
    "e.g. $\\sum_{i=0}^d f(n) = a_in^i$\n",
    "\n",
    "#### 7. The Exponential Function *($f(n) = b^n$)*\n",
    "In exponential function, base ***b*** is a positive constant and the input argument n is the exponent. \\\n",
    "The function f(n) assign to the input n the value obtained by multiplying the base *b* by itself *n* times \\\n",
    "Like Logarithm Function, the most common base is b = 2 \\\n",
    "**Proposition 3.4 (Exponent Rules): (Given positive integers a, b, and c)** \n",
    "1. $(b^a)^c = b^{ac}$\n",
    "2. $b^ab^c = b^{a+c}$\n",
    "3. $b^a/b^c = b^{a-c}$ \n",
    "\n",
    "For example: $ 256 = 16^2 = (2^4)^2 = 2^{4*2} = 2^4*2^4 = 2^{4+4} = 2^{10}/2^2 = 2^{10-2} = 2^8  $ \\\n",
    "\n",
    "given an integer k, we define $b^{1/k}$ to be the $k^{th}$ root of b\n",
    "\n",
    "***Geometric Sums***\n",
    "$\\sum_{i=0}^n a^i = 1 + a^1 + a^2 + a^3 + \\dots + a^n = \\frac{a^{n+1}-1}{a-1}$ \\\n",
    "from the equation above we can see each element is geometrically larger than previous one if a > 0. \\\n",
    "For example : $1 + 2 + 4 + 8 + \\dots + 2^{n-1} = 2^n - 1$\n",
    "\n",
    "\n",
    "#### 3.2.1 Comparing Growth Rates\n",
    "\n",
    "| constant | logarithm  | linear  | n-log-n  | quadratic  | cubic  | exponential |\n",
    "|:--------:|:----------:|:-------:|:--------:|:----------:|:------:|:-----------:|\n",
    "|1 | $logn$| $n$|$nlogn$|$n^2$|$n^3$|$b^n$| \\\n",
    "\n",
    "![Comparing Growth Rates](comparinggrowthrates.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0cfba437d1b95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Asymptotic Analysis\n",
    "In algorithms analysis, it is important to focus on \"big-picture\", for example, it's usually enough to know the running time of an algorithm grows proportionally to n. Which means we will use the method of map the size of input n to value that correspond to the main factor but disregard the constant factors. (e.g. Using Pseudo-code or high-level language implementation may correspond to a small number of primitive operations.)  \\\n",
    "\n",
    "We revisit an example from Chapter 1 to which the goal is finding the largest number: \\\n",
    "```python \n",
    "def find_max(data:list):\n",
    "    # Returning the largest number from a nonempty Python list:     \n",
    "    max_num = data[0]\n",
    "    for var in data:\n",
    "        if var > max_num:\n",
    "            max_num = var\n",
    "    return max_num\n",
    "```\n",
    "This is class example of an algorithm with a running time grows proportionally to n. \\\n",
    "\n",
    "#### 3.3.1 The \"Big-O\" Notation\n",
    "#### *\\#It's an asymptotic way to say \"a function is less than or equal to another function\\#*\n",
    "Pronounce as \"f(n) is big-Oh of g(n)\" \\\n",
    "If there is a real constant c > 0 and an integer constant $n_0 > 1$, then we say : $f(n)$ is $O(g(n))$ \\\n",
    "such that = $f(n) \\le c(g)$, for $ n \\ge n_0$ \\\n",
    "\n",
    "\n",
    "*For a quadratic function as an example, f(n) = 3n^2 + 100n* \\ \n",
    "*It means, for a small number of inputs n, like 1 or 2, the n^2 doesn't be a big matter, until it grows into a big enough number, let's say 10000, then the running time is nearly all depend on the $n^2$ factor but not the 3 or the 100n anymore. \\\n",
    "That is, as $g(n) = n^2$, the $f(n) = 3g(n) + 100n$, with a large enough input, $f(n) = O(g(n))=O(n^2)$, which can also call $f(n)$ is $O(g(n))$ or $f(n)$ is $O(n^2)$.* \\\n",
    "*And all these thinking, is helping us to understand, to calculate the running time (or time complexity), we need to focus on the real matter part, the g(n), which in this case, is $n^2$*\n",
    "\n",
    "#### **Some properties of the Big-Oh Notation**\n",
    "\n",
    "1. $5n^4 + 3n^3 + 2n^2 + 4n + 1$ is $O(n^4)$ # $5n^4+3n^3+2n^2+4n+1 \\le (5+4+3+2+1)n^4 = cn^4$ \\\n",
    "    For $n \\ge 1$  we have $ 1 \\le n \\le n^2 \\le n^3 \\le \\dots \\le n^d $ hence \\\n",
    "    **Justification:** $a_0+a_1n+a_2n^2+a_3n^3+\\dots+a_dn^d \\le (|a_0|+|a_1|+|a_2|+|a_3|+\\dots+|a_d|)n^d$ when $n \\ge 1$ \\\n",
    "    So: $f(n)= O(n^d)$ \\\n",
    "2. $5n^2+3nlogn+2n+5$ is $O(n^2)$. \\\n",
    "    **Justification:** $5n^2+3nlogn+2n+5 \\le (5+3+2+5)n^2 = cn^2$ for $c =15$ when $n \\ge n_0 = 1$ \n",
    "\n",
    "3. $20n^3+10nlogn+5$ is $O(n^3)$ \\\n",
    "    **Justification:** $20n^3 + 10nlogn + 5 \\le 35n^3$ for $n\\ge 1$  \n",
    "\n",
    "4. $3logn + 2$ is $O(n^3)$ \\\n",
    "    **Justification:** $3logn+2 \\le 5logn$, for $n\\ge2$  ($logn = 0$ for n = 1, that's why we use $n\\ge n_0=2$in this case) \\\n",
    "5. $2^{n+2}$ is $O(2^n)$ \\\n",
    "    **Justification:** $2^{n+2} = 2^n * 2^2 = 4 * 2^n$; we can take $c = 4$ and $n_0=1$ in this case. \n",
    "6. $2n+100logn$ is $O(n)$ \\\n",
    "    **Justification:** $2n+100log\\le 102n$, for $n\\ge n_0 = 1$, we can take c = 102 in this case.\n",
    "\n",
    "#### ***==== WE SHOULD ALWAYS KEEP OUR BIG-OH NOTATION ACCURATELY AND SIMPLY AS POSSIBLE! ====*** \n",
    "\n",
    "#### Big-Omega\n",
    "#### *\\#it's an asymptotic way to say \"a function is larger than or equal to another function\" \\#*\n",
    "Pronounce as \"f(n) is big-Omega of g(n)\" \\\n",
    "If there is a real constant c > 0 and an integer constant $n_0 > 1$, then we say : $f(n)$ is $\\Omega(g(n))$ \\\n",
    "such that = $f(n) \\ge c(g)$, for $ n \\ge n_0$\n",
    "#### ** Property of the Big-Omega Notation **\n",
    "1. $3nlogn - 2n $ is $\\Omega(nlogn)$ \\\n",
    "    **Justification:** $3nlogn-2n = nlogn+2n(logn-1) \\ge nlogn$ for $n \\ge 2$; hence, we can take c =1 and $n_0 = 2$ in this case.\n",
    "\n",
    "#### Big-Theta\n",
    "#### *\\#it's an asymptotic way to say \"two functions grow at the same rate, up to constant factor\" \\#*\n",
    "Pronounce as \"f(n) is big-Theta of g(n)\" \\\n",
    "If $f(n)$ is $O(g(n))$ and $f(n)$ is $\\Omega(g(n))$ and there is a real constant $c'\\gt 0$ and $c'' \\gt 0$ an integer constant $n_0 > 1$, then we say : $f(n)$ is $\\Theta(g(n))$, \\\n",
    "such that = $c'g(n) \\le f(n) \\le c''g(n)$, for $n\\ge n_0$\n",
    "\n",
    "#### ** Property of the Big-Theta Notation **\n",
    "1. $3nlogn + 4n + 5logn$ is $\\Theta(nlogn)$.\n",
    "    **Justification:** $3nlogn \\le 3nlogn + 4n+5logn \\le (3+4+5)nlogn$ for $n\\ge 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9221d562df6cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 Simple Justification Techniques\n",
    "#### 3.4.1 By example:\n",
    "- If we need to justify a claim:\" There is an element x in a set S that has property P\"\\\n",
    "    We only need to produce a particular x in set S with property P\n",
    "- If we need to justify a claim:\" Every element x in a set S that has property P\" is False \\\n",
    "    We just need to produce a particular x in set S that does not have property P ***(Counter Example)***\n",
    "\n",
    "#### 3.4.2 The \"Contra\" Attack:\n",
    "#### *DeMorgan's Law*: \n",
    "\"*p* or *q*\"  is \"not *p* and not *q*\" \\\n",
    "\"*p* and *q*\" is \"not *p* or not *q*\".\n",
    "\n",
    "***Contrapositive*** statement: \"If *p* is true, then *q* is true\" == \"If *q* is not true, then *p* is not true.\" Which mean the first statement is the contrapositive of the second statement. \n",
    "\n",
    "***Contradiction*** If there is a statement that *p* is true, then supposing that *q* is false and then showing that this assumption leads to a contradiction (such as $2≠2$ or $1\\gt3$). By reaching such a contradiction, we show that no consistent situation exists with *q* being false, so *q* must be true.\n",
    "\n",
    "**Example:** \"Let *a* and *b* be integers, if *ab* is even, then *a* is even or *b* is even.\"\n",
    "\n",
    "**Justification 1:** *(In contrapositive way)* \n",
    "1. Change the statemenet into: \"If *a* and *b* are odds, then *ab* is odd.\"\n",
    "2. Suppose $a=2j+1$ and $b=2k+1$ for some integer\n",
    "3. Then $ab = 4jk+2j+2k+1 = 2(2jk+j+k)+1$ hence, *ab* is odd \n",
    "\n",
    "**Example:** \"Let *a* and *b* be integers, if *ab* is odd, then *a* is odd and *b* is odd.\"\n",
    "\n",
    "**Justification 2:** *(In contradiction way)*\n",
    "1. Let *ab* be odd (and we with to show that *a* is odd and *b* is odd).\n",
    "2. Assuming the opposite: supposing *a* is even or *b* is even\n",
    "3. Then $a = 2j$ for some integer *j*\n",
    "4. Hence $ab=(2j)b=2(jb)$ that is *ab* is even which is a contradiction given *ab* can't be simlutaneously.\n",
    "5. Therefore, a is odd and b is odd.\n",
    "\n",
    "#### 3.4.3 Induction and Loop invariants\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA_Learning_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
